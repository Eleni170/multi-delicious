{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/vocabs.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bdeeaf41b98f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mvocabs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"vocabs.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m', '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/vocabs.txt'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "path = \"../Data/\"\n",
    "words = []\n",
    "\n",
    "\n",
    "def train_to_sentence_form(path):\n",
    "    # reads the file to a format of docs[doc[[sentence1],[sentence2]],...]\n",
    "    docs=[]\n",
    "    num_of_sentences = []\n",
    "    with open(path) as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            line = line.rstrip('\\n').split(\" \")\n",
    "            #print(line)\n",
    "            num = line.pop(0)\n",
    "            #print(num)\n",
    "            number = int(num[1:-1])\n",
    "            num_of_sentences.append(number)\n",
    "            len_of_sentence = []\n",
    "\n",
    "            for i,k in enumerate(line):\n",
    "                if k.startswith('<'):\n",
    "                    k=k[1:-1]\n",
    "                    len_of_sentence.append(int(k))\n",
    "                    line.pop(i)\n",
    "            doc_sentence=[]       \n",
    "            sentence=[]\n",
    "            for j in len_of_sentence:\n",
    "                sentence = line[:j]\n",
    "                line=line[j:]\n",
    "                docs.append( sentence) \n",
    "            line = f.readline()\n",
    "    print(len(docs))\n",
    "    return docs,num_of_sentences     \n",
    "    #docs is a list of lists where each list is a different sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocabs = open(path+\"vocabs.txt\").readlines()\n",
    "for vocab in vocabs:\n",
    "    word, _ = vocab.split(', ')\n",
    "    words.append(word)\n",
    "\n",
    "train_label = [i.strip() for i in open(path+\"train-label.dat\").readlines()]\n",
    "train_label = np.array(train_label)\n",
    "\n",
    "#path=\"../Data/\"\n",
    "docs,num_sentences = train_to_sentence_form(path+\"train-data.dat\")\n",
    "\n",
    "#train_data = convert_to_word(path + \"/train-data.dat\")\n",
    "data = []\n",
    "for i in docs:\n",
    "    final_L = ''\n",
    "    for l in i:\n",
    "        final_L = final_L + ' ' + words[int(l)]\n",
    "    data.append(final_L[1:])\n",
    "print(data[:12])\n",
    "print(len(data))       \n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(data)\n",
    "print(X_train_counts.size)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_confusion_matrix(predict,path,clustnum):\n",
    "    test_label = [i.strip().split() for i in open(path + \"test-label.dat\").readlines()]\n",
    "\n",
    "    for i in range(len(test_label)):\n",
    "        item = test_label[i]\n",
    "        for j in range(len(item)):\n",
    "            item[j] = int(item[j])\n",
    "\n",
    "        test_label[i] = item\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for item in open(path + \"labels.txt\").readlines():\n",
    "        item = item.replace('\\n', '')\n",
    "        label_name, _ = item.split(', ')\n",
    "        labels.append(label_name)\n",
    "\n",
    "    y_true = np.array(test_label)\n",
    "    y_pred = np.array(predict)\n",
    "\n",
    "    conf_mat_dict = {}\n",
    "\n",
    "    for label_col in range(len(labels)):\n",
    "        y_true_label = y_true[:, label_col]\n",
    "        y_pred_label = y_pred[:, label_col]\n",
    "        conf_mat_dict[labels[label_col]] = confusion_matrix(y_pred=y_pred_label, y_true=y_true_label)\n",
    "    f = open(\"confmatrix\"+str(clustnum)+\".txt\", \"w\")\n",
    "    f.close()\n",
    "    for label, matrix in conf_mat_dict.items():\n",
    "        print(\"Confusion matrix for label {}:\".format(label))\n",
    "        f = open(\"confmatrix\"+str(clustnum)+\".txt\", \"a\")\n",
    "        print(matrix)\n",
    "        f.write(\"Confusion matrix for label {}:\".format(label))\n",
    "        np.savetxt(f,matrix)\n",
    "        f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8251\n",
      "149925\n",
      "train\n",
      "73363\n",
      "fitting\n",
      "73363\n",
      "['0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0'\n",
      " '0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0'\n",
      " '0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1' ...\n",
      " '0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0'\n",
      " '0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0'\n",
      " '0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0']\n",
      "Confusion matrix for label programming:\n",
      "[[2353  653]\n",
      " [ 802  175]]\n",
      "Confusion matrix for label style:\n",
      "[[3445  310]\n",
      " [ 207   21]]\n",
      "Confusion matrix for label reference:\n",
      "[[ 894 1531]\n",
      " [ 594  964]]\n",
      "Confusion matrix for label java:\n",
      "[[2548 1063]\n",
      " [ 302   70]]\n",
      "Confusion matrix for label web:\n",
      "[[2191  742]\n",
      " [ 752  298]]\n",
      "Confusion matrix for label internet:\n",
      "[[2828  618]\n",
      " [ 424  113]]\n",
      "Confusion matrix for label culture:\n",
      "[[3025  256]\n",
      " [ 635   67]]\n",
      "Confusion matrix for label design:\n",
      "[[1989  915]\n",
      " [ 764  315]]\n",
      "Confusion matrix for label education:\n",
      "[[2337  843]\n",
      " [ 633  170]]\n",
      "Confusion matrix for label language:\n",
      "[[2757  743]\n",
      " [ 387   96]]\n",
      "Confusion matrix for label books:\n",
      "[[3259  217]\n",
      " [ 470   37]]\n",
      "Confusion matrix for label writing:\n",
      "[[3078  427]\n",
      " [ 411   67]]\n",
      "Confusion matrix for label computer:\n",
      "[[3217  257]\n",
      " [ 463   46]]\n",
      "Confusion matrix for label english:\n",
      "[[3108  520]\n",
      " [ 305   50]]\n",
      "Confusion matrix for label politics:\n",
      "[[3418  173]\n",
      " [ 367   25]]\n",
      "Confusion matrix for label history:\n",
      "[[3415  127]\n",
      " [ 425   16]]\n",
      "Confusion matrix for label philosophy:\n",
      "[[3706    8]\n",
      " [ 268    1]]\n",
      "Confusion matrix for label science:\n",
      "[[3295  187]\n",
      " [ 477   24]]\n",
      "Confusion matrix for label religion:\n",
      "[[3745   31]\n",
      " [ 205    2]]\n",
      "Confusion matrix for label grammar:\n",
      "[[3313  537]\n",
      " [ 116   17]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# KMeans clustering a kind of clustering.\n",
    "from sklearn.cluster import KMeans\n",
    "print(train_label.size)\n",
    "number_of_clusters_list = [500]\n",
    "for number_of_clusters in number_of_clusters_list:\n",
    "    km = KMeans(n_clusters=number_of_clusters)\n",
    "    # fit the matrix\n",
    "    km.fit(X_train_tfidf)\n",
    "    initform = np.zeros([train_label.size,number_of_clusters])\n",
    "    df = pd.DataFrame(initform,columns=[\"Cluster \"+ str(i) for i in range(number_of_clusters)])\n",
    "    print(km.labels_.size)\n",
    "    clust_labels = km.labels_\n",
    "    acc = 0\n",
    "    for i,val in enumerate(num_sentences):\n",
    "        for j in range(val):\n",
    "            df.iloc[i][clust_labels[acc]] += 1\n",
    "            acc+=1\n",
    "\n",
    "    df.head(20)\n",
    "\n",
    "\n",
    "    print(\"train\")\n",
    "    from sklearn import neighbors\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=3 ,weights='distance',metric='minkowski',p=1).fit(df, train_label)\n",
    "\n",
    "    test_data,test_sentences = train_to_sentence_form(path+\"test-data.dat\")\n",
    "    test_label = [i.strip() for i in open(path+\"test-label.dat\").readlines()]\n",
    "    test_label = np.array(test_label)\n",
    "\n",
    "    data2 = []\n",
    "    for i in test_data:\n",
    "        final_L = ''\n",
    "        for l in i:\n",
    "            final_L = final_L + ' ' + words[int(l)]\n",
    "        data2.append(final_L[1:])\n",
    "\n",
    "    X_test_counts = count_vect.transform(data2)\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "    print(\"fitting\")\n",
    "    km.fit(X_test_tfidf)\n",
    "    print(km.labels_.size)\n",
    "\n",
    "    testform = np.zeros([test_label.size,number_of_clusters])\n",
    "    df_test = pd.DataFrame(testform,columns=[\"Cluster \"+ str(i) for i in range(number_of_clusters)])\n",
    "\n",
    "    clusttest_labels = km.labels_\n",
    "    acctest = 0\n",
    "    for i,val in enumerate(test_sentences):\n",
    "        for j in range(val):\n",
    "            df_test.iloc[i][clusttest_labels[acctest]] += 1\n",
    "            acctest+=1\n",
    "\n",
    "    df_test.head(20)\n",
    "\n",
    "    pred = clf.predict(df_test)\n",
    "    print(pred)\n",
    "    \n",
    "    predict=[]\n",
    "    for p in pred:\n",
    "        result = p.split()\n",
    "        for r in range(len(result)):\n",
    "            result[r] = int(result[r])\n",
    "        predict.append(result)\n",
    "    \n",
    "    multilabel_confusion_matrix(predict,path,number_of_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
