{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149925\n",
      "['rubi rail helper demo more info auto complet', 'see new helper action', 'null length substr locat', 'exec messag messag pleas edit remov follow word content', 'roll stone com news song previou next page', 'good vibrat beach boy', 'smell teen spirit nirvana', 'want hold hand beatl', 'hound dog elvi', 'god know beach boy', 'walk line johnni cash', 'heaven led zeppelin']\n",
      "149925\n",
      "965852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#path = \"../../Data/\"\n",
    "words = []\n",
    "\n",
    "\n",
    "def train_to_sentence_form(path):\n",
    "    # reads the file to a format of docs[doc[[sentence1],[sentence2]],...]\n",
    "    docs=[]\n",
    "    num_of_sentences = []\n",
    "    with open(path) as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            line = line.rstrip('\\n').split(\" \")\n",
    "            #print(line)\n",
    "            num = line.pop(0)\n",
    "            #print(num)\n",
    "            number = int(num[1:-1])\n",
    "            num_of_sentences.append(number)\n",
    "            len_of_sentence = []\n",
    "\n",
    "            for i,k in enumerate(line):\n",
    "                if k.startswith('<'):\n",
    "                    k=k[1:-1]\n",
    "                    len_of_sentence.append(int(k))\n",
    "                    line.pop(i)\n",
    "            doc_sentence=[]       \n",
    "            sentence=[]\n",
    "            for j in len_of_sentence:\n",
    "                sentence = line[:j]\n",
    "                line=line[j:]\n",
    "                docs.append( sentence) \n",
    "            line = f.readline()\n",
    "    print(len(docs))\n",
    "    return docs,num_of_sentences     \n",
    "    #docs is a list of lists where each list is a different sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocabs = open(\"vocabs.txt\").readlines()\n",
    "for vocab in vocabs:\n",
    "    word, _ = vocab.split(', ')\n",
    "    words.append(word)\n",
    "\n",
    "train_label = [i.strip() for i in open(\"train-label.dat\").readlines()]\n",
    "train_label = np.array(train_label)\n",
    "\n",
    "#path=\"../Data/\"\n",
    "docs,num_sentences = train_to_sentence_form(\"train-data.dat\")\n",
    "\n",
    "#train_data = convert_to_word(path + \"/train-data.dat\")\n",
    "data = []\n",
    "for i in docs:\n",
    "    final_L = ''\n",
    "    for l in i:\n",
    "        final_L = final_L + ' ' + words[int(l)]\n",
    "    data.append(final_L[1:])\n",
    "print(data[:12])\n",
    "print(len(data))       \n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(data)\n",
    "print(X_train_counts.size)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "svd = TruncatedSVD(n_components=20,n_iter=5, random_state=1)\n",
    "mat_reduced = svd.fit_transform(X_train_tfidf)\n",
    "Data_new = np.array(mat_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_confusion_matrix(predict,clustnum):\n",
    "    test_label = [i.strip().split() for i in open( \"test-label.dat\").readlines()]\n",
    "\n",
    "    for i in range(len(test_label)):\n",
    "        item = test_label[i]\n",
    "        for j in range(len(item)):\n",
    "            item[j] = int(item[j])\n",
    "\n",
    "        test_label[i] = item\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for item in open (\"labels.txt\").readlines():\n",
    "        item = item.replace('\\n', '')\n",
    "        label_name, _ = item.split(', ')\n",
    "        labels.append(label_name)\n",
    "\n",
    "    y_true = np.array(test_label)\n",
    "    y_pred = np.array(predict)\n",
    "\n",
    "    conf_mat_dict = {}\n",
    "\n",
    "    for label_col in range(len(labels)):\n",
    "        y_true_label = y_true[:, label_col]\n",
    "        y_pred_label = y_pred[:, label_col]\n",
    "        conf_mat_dict[labels[label_col]] = confusion_matrix(y_pred=y_pred_label, y_true=y_true_label)\n",
    "    f = open(\"confmatrix\"+str(clustnum)+\".txt\", \"w\")\n",
    "    f.close()\n",
    "    for label, matrix in conf_mat_dict.items():\n",
    "        print(\"Confusion matrix for label {}:\".format(label))\n",
    "        f = open(\"confmatrix\"+str(clustnum)+\".txt\", \"a\")\n",
    "        print(matrix)\n",
    "        f.write(\"Confusion matrix for label {}:\".format(label))\n",
    "        np.savetxt(f,matrix)\n",
    "        f.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n",
      "149925\n",
      "train\n",
      "73363\n",
      "[ 7 17  5 ... 18 14 17]\n",
      "['1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0'\n",
      " '0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0'\n",
      " '1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0' ...\n",
      " '0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0'\n",
      " '0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0'\n",
      " '0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0']\n",
      "Confusion matrix for label programming:\n",
      "[[2709  297]\n",
      " [ 781  196]]\n",
      "Confusion matrix for label style:\n",
      "[[3614  141]\n",
      " [ 214   14]]\n",
      "Confusion matrix for label reference:\n",
      "[[1860  565]\n",
      " [1099  459]]\n",
      "Confusion matrix for label java:\n",
      "[[3410  201]\n",
      " [ 320   52]]\n",
      "Confusion matrix for label web:\n",
      "[[2482  451]\n",
      " [ 761  289]]\n",
      "Confusion matrix for label internet:\n",
      "[[3088  358]\n",
      " [ 459   78]]\n",
      "Confusion matrix for label culture:\n",
      "[[2758  523]\n",
      " [ 548  154]]\n",
      "Confusion matrix for label design:\n",
      "[[2297  607]\n",
      " [ 803  276]]\n",
      "Confusion matrix for label education:\n",
      "[[2630  550]\n",
      " [ 651  152]]\n",
      "Confusion matrix for label language:\n",
      "[[3082  418]\n",
      " [ 415   68]]\n",
      "Confusion matrix for label books:\n",
      "[[3046  430]\n",
      " [ 432   75]]\n",
      "Confusion matrix for label writing:\n",
      "[[3164  341]\n",
      " [ 406   72]]\n",
      "Confusion matrix for label computer:\n",
      "[[3134  340]\n",
      " [ 436   73]]\n",
      "Confusion matrix for label english:\n",
      "[[3328  300]\n",
      " [ 310   45]]\n",
      "Confusion matrix for label politics:\n",
      "[[3210  381]\n",
      " [ 322   70]]\n",
      "Confusion matrix for label history:\n",
      "[[3171  371]\n",
      " [ 375   66]]\n",
      "Confusion matrix for label philosophy:\n",
      "[[3432  282]\n",
      " [ 223   46]]\n",
      "Confusion matrix for label science:\n",
      "[[3045  437]\n",
      " [ 425   76]]\n",
      "Confusion matrix for label religion:\n",
      "[[3538  238]\n",
      " [ 171   36]]\n",
      "Confusion matrix for label grammar:\n",
      "[[3694  156]\n",
      " [ 122   11]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# KMeans clustering a kind of clustering.\n",
    "from sklearn.cluster import KMeans\n",
    "#print(train_label.size)\n",
    "number_of_clusters_list = [20]\n",
    "for number_of_clusters in number_of_clusters_list:\n",
    "    km = KMeans(n_clusters=number_of_clusters)\n",
    "    # fit the matrix\n",
    "    print(\"fitting\")\n",
    "    km = km.fit(Data_new)\n",
    "    initform = np.zeros([train_label.size,number_of_clusters])\n",
    "    df = pd.DataFrame(initform,columns=[\"Cluster \"+ str(i) for i in range(number_of_clusters)])\n",
    "    print(km.labels_.size)\n",
    "    clust_labels = km.labels_\n",
    "    acc = 0\n",
    "    for i,val in enumerate(num_sentences):\n",
    "        for j in range(val):\n",
    "            df.iloc[i][clust_labels[acc]] += 1\n",
    "            acc+=1\n",
    "\n",
    "    df.head(20)\n",
    "\n",
    "\n",
    "    print(\"train\")\n",
    "    from sklearn import neighbors\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=3 ,weights='distance',metric='minkowski',p=1).fit(df, train_label)\n",
    "\n",
    "    test_data,test_sentences = train_to_sentence_form(\"test-data.dat\")\n",
    "    test_label = [i.strip() for i in open(\"test-label.dat\").readlines()]\n",
    "    test_label = np.array(test_label)\n",
    "\n",
    "    data2 = []\n",
    "    for i in test_data:\n",
    "        final_L = ''\n",
    "        for l in i:\n",
    "            final_L = final_L + ' ' + words[int(l)]\n",
    "        data2.append(final_L[1:])\n",
    "\n",
    "    X_test_counts = count_vect.transform(data2)\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "    mat_reduced = svd.transform(X_test_tfidf)\n",
    "    Data_new_test = np.array(mat_reduced)\n",
    "    #print(\"fitting\")\n",
    "    #km.fit(X_test_tfidf)\n",
    "    #print(km.labels_.size)\n",
    "    clusttest_labels = km.predict(Data_new_test)\n",
    "    print(clusttest_labels)\n",
    "    testform = np.zeros([test_label.size,number_of_clusters])\n",
    "    df_test = pd.DataFrame(testform,columns=[\"Cluster \"+ str(i) for i in range(number_of_clusters)])\n",
    "\n",
    "    #clusttest_labels = km.labels_\n",
    "    acctest = 0\n",
    "    for i,val in enumerate(test_sentences):\n",
    "        for j in range(val):\n",
    "            df_test.iloc[i][clusttest_labels[acctest]] += 1\n",
    "            acctest+=1\n",
    "\n",
    "    df_test.head(20)\n",
    "\n",
    "    pred = clf.predict(df_test)\n",
    "    print(pred)\n",
    "    \n",
    "    predict=[]\n",
    "    for p in pred:\n",
    "        result = p.split()\n",
    "        for r in range(len(result)):\n",
    "            result[r] = int(result[r])\n",
    "        predict.append(result)\n",
    "    \n",
    "    multilabel_confusion_matrix(predict,number_of_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"train_kmeans_svd.csv\",index=False)\n",
    "df_test.to_csv(\"test_kmeanssvd.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
