{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149925\n",
      "['rubi rail helper demo more info auto complet', 'see new helper action', 'null length substr locat', 'exec messag messag pleas edit remov follow word content', 'roll stone com news song previou next page', 'good vibrat beach boy', 'smell teen spirit nirvana', 'want hold hand beatl', 'hound dog elvi', 'god know beach boy', 'walk line johnni cash', 'heaven led zeppelin']\n",
      "149925\n",
      "965852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "path = \"../Data/\"\n",
    "words = []\n",
    "\n",
    "\n",
    "def train_to_sentence_form(path):\n",
    "    # reads the file to a format of docs[doc[[sentence1],[sentence2]],...]\n",
    "    docs=[]\n",
    "    num_of_sentences = []\n",
    "    with open(path) as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            line = line.rstrip('\\n').split(\" \")\n",
    "            #print(line)\n",
    "            num = line.pop(0)\n",
    "            #print(num)\n",
    "            number = int(num[1:-1])\n",
    "            num_of_sentences.append(number)\n",
    "            len_of_sentence = []\n",
    "\n",
    "            for i,k in enumerate(line):\n",
    "                if k.startswith('<'):\n",
    "                    k=k[1:-1]\n",
    "                    len_of_sentence.append(int(k))\n",
    "                    line.pop(i)\n",
    "            doc_sentence=[]       \n",
    "            sentence=[]\n",
    "            for j in len_of_sentence:\n",
    "                sentence = line[:j]\n",
    "                line=line[j:]\n",
    "                docs.append( sentence) \n",
    "            line = f.readline()\n",
    "    print(len(docs))\n",
    "    return docs,num_of_sentences     \n",
    "    #docs is a list of lists where each list is a different sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocabs = open(path+\"vocabs.txt\").readlines()\n",
    "for vocab in vocabs:\n",
    "    word, _ = vocab.split(', ')\n",
    "    words.append(word)\n",
    "\n",
    "train_label = [i.strip() for i in open(path+\"train-label.dat\").readlines()]\n",
    "train_label = np.array(train_label)\n",
    "\n",
    "#path=\"../Data/\"\n",
    "docs,num_sentences = train_to_sentence_form(path+\"train-data.dat\")\n",
    "\n",
    "#train_data = convert_to_word(path + \"/train-data.dat\")\n",
    "data = []\n",
    "for i in docs:\n",
    "    final_L = ''\n",
    "    for l in i:\n",
    "        final_L = final_L + ' ' + words[int(l)]\n",
    "    data.append(final_L[1:])\n",
    "print(data[:12])\n",
    "print(len(data))       \n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(data)\n",
    "print(X_train_counts.size)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_confusion_matrix(predict,path,clustnum=-3):\n",
    "    test_label = [i.strip().split() for i in open(path + \"test-label.dat\").readlines()]\n",
    "\n",
    "    for i in range(len(test_label)):\n",
    "        item = test_label[i]\n",
    "        for j in range(len(item)):\n",
    "            item[j] = int(item[j])\n",
    "\n",
    "        test_label[i] = item\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for item in open(path + \"labels.txt\").readlines():\n",
    "        item = item.replace('\\n', '')\n",
    "        label_name, _ = item.split(', ')\n",
    "        labels.append(label_name)\n",
    "\n",
    "    y_true = np.array(test_label)\n",
    "    y_pred = np.array(predict)\n",
    "\n",
    "    conf_mat_dict = {}\n",
    "\n",
    "    for label_col in range(len(labels)):\n",
    "        y_true_label = y_true[:, label_col]\n",
    "        y_pred_label = y_pred[:, label_col]\n",
    "        conf_mat_dict[labels[label_col]] = confusion_matrix(y_pred=y_pred_label, y_true=y_true_label)\n",
    "    f = open(\"confmatrix\"+str(clustnum)+\".txt\", \"w\")\n",
    "    f.close()\n",
    "    for label, matrix in conf_mat_dict.items():\n",
    "        print(\"Confusion matrix for label {}:\".format(label))\n",
    "        f = open(\"confmatrix\"+str(clustnum)+\".txt\", \"a\")\n",
    "        print(matrix)\n",
    "        f.write(\"Confusion matrix for label {}:\".format(label))\n",
    "        np.savetxt(f,matrix)\n",
    "        f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73363\n",
      "(6362, 21)\n",
      "(4442, 22)\n",
      "(4406, 23)\n",
      "(4100, 24)\n",
      "(3118, 25)\n",
      "(2942, 26)\n",
      "(2422, 27)\n",
      "(2098, 28)\n",
      "(2068, 29)\n",
      "(2008, 30)\n",
      "(2002, 31)\n",
      "(1878, 32)\n",
      "(1796, 33)\n",
      "(1660, 34)\n",
      "(1598, 35)\n",
      "(1450, 36)\n",
      "(1196, 37)\n",
      "(958, 38)\n",
      "(822, 39)\n",
      "(448, 40)\n",
      "Confusion matrix for label programming:\n",
      "[[1658 1348]\n",
      " [ 504  473]]\n",
      "Confusion matrix for label style:\n",
      "[[1932 1823]\n",
      " [ 112  116]]\n",
      "Confusion matrix for label reference:\n",
      "[[1287 1138]\n",
      " [ 861  697]]\n",
      "Confusion matrix for label java:\n",
      "[[1816 1795]\n",
      " [ 169  203]]\n",
      "Confusion matrix for label web:\n",
      "[[1601 1332]\n",
      " [ 547  503]]\n",
      "Confusion matrix for label internet:\n",
      "[[1753 1693]\n",
      " [ 276  261]]\n",
      "Confusion matrix for label culture:\n",
      "[[1639 1642]\n",
      " [ 352  350]]\n",
      "Confusion matrix for label design:\n",
      "[[1512 1392]\n",
      " [ 591  488]]\n",
      "Confusion matrix for label education:\n",
      "[[1464 1716]\n",
      " [ 360  443]]\n",
      "Confusion matrix for label language:\n",
      "[[1667 1833]\n",
      " [ 255  228]]\n",
      "Confusion matrix for label books:\n",
      "[[1829 1647]\n",
      " [ 272  235]]\n",
      "Confusion matrix for label writing:\n",
      "[[1710 1795]\n",
      " [ 246  232]]\n",
      "Confusion matrix for label computer:\n",
      "[[1670 1804]\n",
      " [ 253  256]]\n",
      "Confusion matrix for label english:\n",
      "[[1665 1963]\n",
      " [ 165  190]]\n",
      "Confusion matrix for label politics:\n",
      "[[1885 1706]\n",
      " [ 188  204]]\n",
      "Confusion matrix for label history:\n",
      "[[1759 1783]\n",
      " [ 215  226]]\n",
      "Confusion matrix for label philosophy:\n",
      "[[2109 1605]\n",
      " [ 127  142]]\n",
      "Confusion matrix for label science:\n",
      "[[1726 1756]\n",
      " [ 245  256]]\n",
      "Confusion matrix for label religion:\n",
      "[[1986 1790]\n",
      " [  92  115]]\n",
      "Confusion matrix for label grammar:\n",
      "[[1615 2235]\n",
      " [  68   65]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "# KMeans clustering a kind of clustering.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "test_data,test_sentences = train_to_sentence_form(path+\"test-data.dat\")\n",
    "test_label = [i.strip() for i in open(path+\"test-label.dat\").readlines()]\n",
    "test_label = np.array(test_label)\n",
    "train_label = [i.strip() for i in open(path+\"train-label.dat\").readlines()]\n",
    "train_label = np.array(train_label)\n",
    "\n",
    "df = pd.read_csv(\"train_kmeans_.csv\") \n",
    "df_test = pd.read_csv(\"test_kmeans_.csv\")\n",
    "#train_label = pd.DataFrame(data = train_label)\n",
    "labels_models=[]\n",
    "for i in range (0,20 ):\n",
    "    tr=[]\n",
    "    for j in range (0,len(train_label)):\n",
    "        tr.append(int(train_label[j][2*i]))\n",
    "    labels_models.append(tr)\n",
    "    \n",
    "train_labels = pd.DataFrame(labels_models)\n",
    "train_labels = train_labels.transpose()\n",
    "#print(train_labels[:][2])\n",
    "labels__models=[]\n",
    "for i in range (0,20 ):\n",
    "    ts=[]\n",
    "    for j in range (0,len(test_label)):\n",
    "        ts.append(int(test_label[j][2*i]))\n",
    "    labels__models.append(ts)\n",
    "    \n",
    "test_labels = pd.DataFrame(labels__models)\n",
    "test_labels = test_labels.transpose()\n",
    "\n",
    "from sklearn import neighbors\n",
    "pred = pd.DataFrame()\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=2)\n",
    "rm = RandomUnderSampler(random_state=42)\n",
    "dfinit = df.copy()\n",
    "orderlist = [2,7,4,0,8,6,5,12,10,9,17,11,15,14,3,13,16,1,18,19]\n",
    "for l in orderlist:\n",
    "    #if 4 * train_labels[train_labels[:][l] == 1].size < train_labels[train_labels[:][l] == 0].size:\n",
    "    df, train_labels_new = rm.fit_resample(dfinit,train_labels[:][l])\n",
    "    #else:\n",
    "    #df, train_labels_new = sm.fit_resample(dfinit, train_labels[:][l])\n",
    "    print(df.shape)\n",
    "    KNN = neighbors.KNeighborsClassifier(n_neighbors=3 ,weights='distance',metric='minkowski',p=1).fit(df, train_labels_new)\n",
    "    dfinit['Label'+str(l)] = train_labels[:][l]\n",
    "    df_test['Label'+str(l)] = KNN.predict(df_test) \n",
    "    pred['Label'+str(l)] = df_test['Label'+str(l)]\n",
    "    \n",
    "\n",
    "#clustnum = -1 smote\n",
    "#-2 randomundersampler\n",
    "'''sm = SMOTE(sampling_strategy='not majority', random_state=2)\n",
    "df, train_labels = sm.fit_resample(df, train_labels)\n",
    "\n",
    "\n",
    "KNN = neighbors.KNeighborsClassifier(n_neighbors=3 ,weights='distance',metric='minkowski',p=1).fit(df, train_labels)'''\n",
    "\n",
    "\n",
    "#pred = KNN.predict(df_test)\n",
    "#print(pred)\n",
    "predict=[]\n",
    "for p in range(3983):\n",
    "    #print(p)\n",
    "    result = []\n",
    "    for r in range(20):\n",
    "        x = orderlist.index(r)\n",
    "        result.append(pred.iloc[p,x])\n",
    "    predict.append(result)\n",
    "    \n",
    "multilabel_confusion_matrix(predict,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
